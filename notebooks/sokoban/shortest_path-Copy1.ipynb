{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/harsh/Work/habitat-sim/')\n",
    "sys.path.append('/Users/harsh/Work/habitat-api/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/pskynet3/hagrawal9/project/sokoban/habitat-api\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import habitat\n",
    "import habitat_sim\n",
    "import habitat_sim.bindings as hsim\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import magnum as mn\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm \n",
    "\n",
    "import habitat_sim\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "from habitat import Config, logger\n",
    "from habitat import make_dataset\n",
    "from habitat.core.simulator import AgentState\n",
    "# from habitat.tasks.nav.shortest_path_follower import ShortestPathFollower\n",
    "from habitat.sims.habitat_simulator.actions import HabitatSimActions\n",
    "from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim\n",
    "from habitat.utils.visualizations.utils import observations_to_image, images_to_video\n",
    "from habitat.utils.geometry_utils import (\n",
    "    angle_between_quaternions,\n",
    "    quaternion_from_two_vectors,\n",
    ")\n",
    "from habitat.tasks.utils import (\n",
    "    cartesian_to_polar,\n",
    "    quaternion_from_coeff,\n",
    "    quaternion_rotate_vector\n",
    ")\n",
    "\n",
    "from habitat_baselines.common.base_trainer import BaseRLTrainer\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "from habitat_baselines.common.env_utils import construct_envs\n",
    "from habitat_baselines.common.environments import get_env_class, SokobanRLEnv\n",
    "from habitat_baselines.common.rollout_storage import RolloutStorage\n",
    "from habitat_baselines.common.tensorboard_utils import TensorboardWriter\n",
    "from habitat_baselines.common.utils import (\n",
    "    batch_obs,\n",
    "    generate_video,\n",
    "    linear_decay,\n",
    ")\n",
    "from habitat_baselines.config.default import get_config\n",
    "from habitat_baselines.rl.ppo import PPO, PointNavBaselinePolicy\n",
    "\n",
    "from habitat_sim.physics import MotionType\n",
    "from habitat_sim.attributes import PhysicsObjectAttributes\n",
    "from habitat_sim.helper import *\n",
    "\n",
    "\n",
    "EPSILON = 1e-6\n",
    "\n",
    "\n",
    "def action_to_one_hot(action: int) -> np.array:\n",
    "    one_hot = np.zeros(len(HabitatSimActions), dtype=np.float32)\n",
    "    one_hot[action] = 1\n",
    "    return one_hot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = \"data/datasets/sokoban/coda/v1/train/content/val_object_5_scene_10_coda.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"habitat_baselines/config/sokoban/pickup_order_ddppo_obj_5_scene_100.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.defrost()\n",
    "config.SENSORS = ['RGB_SENSOR', 'DEPTH_SENSOR']\n",
    "config.NUM_PROCESSES = 1\n",
    "config.TASK_CONFIG.TASK.SENSORS = ['GRIPPED_OBJECT_SENSOR',\n",
    " 'CLOSEST_OBJECT_SENSOR']\n",
    "config.TASK_CONFIG.ENVIRONMENT.MAX_EPISODE_STEPS = 1000\n",
    "config.TASK_CONFIG.TASK.POSSIBLE_ACTIONS = ['STOP', 'MOVE_FORWARD', 'TURN_LEFT', 'TURN_RIGHT', 'GRAB_RELEASE']\n",
    "config.TASK_CONFIG.DATASET.CONTENT_SCENES = ['val_object_5_scene_10_coda']\n",
    "config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.SHUFFLE = False\n",
    "config.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-30 18:47:44,153 Initializing dataset Sokoban-v0\n"
     ]
    }
   ],
   "source": [
    "dataset = make_dataset(config.TASK_CONFIG.DATASET.TYPE, config=config.TASK_CONFIG.DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortestPathFollower:\n",
    "    r\"\"\"Utility class for extracting the action on the shortest path to the\n",
    "        goal.\n",
    "    Args:\n",
    "        sim: HabitatSim instance.\n",
    "        goal_radius: Distance between the agent and the goal for it to be\n",
    "            considered successful.\n",
    "        return_one_hot: If true, returns a one-hot encoding of the action\n",
    "            (useful for training ML agents). If false, returns the\n",
    "            SimulatorAction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, sim: HabitatSim, goal_radius: float, return_one_hot: bool = True\n",
    "    ):\n",
    "        assert (\n",
    "            getattr(sim, \"geodesic_distance\", None) is not None\n",
    "        ), \"{} must have a method called geodesic_distance\".format(\n",
    "            type(sim).__name__\n",
    "        )\n",
    "\n",
    "        self._sim = sim\n",
    "        self._max_delta = self._sim.config.FORWARD_STEP_SIZE - EPSILON\n",
    "        self._goal_radius = goal_radius\n",
    "        self._step_size = self._sim.config.FORWARD_STEP_SIZE\n",
    "\n",
    "        self._mode = (\n",
    "            \"exact_gradient\"\n",
    "            if getattr(sim, \"get_straight_shortest_path_points\", None)\n",
    "            is not None\n",
    "            else \"approximate_gradient\"\n",
    "        )\n",
    "        \n",
    "        self._mode = \"approximate_gradient\"\n",
    "        \n",
    "        self._return_one_hot = return_one_hot\n",
    "\n",
    "    def _get_return_value(self, action) -> Union[int, np.array]:\n",
    "        if self._return_one_hot:\n",
    "            return action_to_one_hot(action)\n",
    "        else:\n",
    "            return action\n",
    "\n",
    "    def get_next_action(\n",
    "        self, goal_pos: np.array\n",
    "    ) -> Optional[Union[int, np.array]]:\n",
    "        \"\"\"Returns the next action along the shortest path.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self._sim.geodesic_distance(\n",
    "                self._sim.get_agent_state().position, [goal_pos]\n",
    "            )\n",
    "            <= self._goal_radius\n",
    "        ):\n",
    "            return None\n",
    "\n",
    "        max_grad_dir = self._est_max_grad_dir(goal_pos)\n",
    "        \n",
    "        if max_grad_dir is None:\n",
    "            return self._get_return_value(HabitatSimActions.MOVE_FORWARD)\n",
    "            # return None\n",
    "        \n",
    "        return self._step_along_grad(max_grad_dir)\n",
    "\n",
    "    def _step_along_grad(\n",
    "        self, grad_dir: np.quaternion\n",
    "    ) -> Union[int, np.array]:\n",
    "        current_state = self._sim.get_agent_state()\n",
    "\n",
    "        alpha = angle_between_quaternions(grad_dir, current_state.rotation)\n",
    "        if alpha <= np.deg2rad(self._sim.config.TURN_ANGLE) + EPSILON:\n",
    "            return self._get_return_value(HabitatSimActions.MOVE_FORWARD)\n",
    "        else:\n",
    "            sim_action = HabitatSimActions.TURN_LEFT\n",
    "            self._sim.step(sim_action)\n",
    "            best_turn = (\n",
    "                HabitatSimActions.TURN_LEFT\n",
    "                if (\n",
    "                    angle_between_quaternions(\n",
    "                        grad_dir, self._sim.get_agent_state().rotation\n",
    "                    )\n",
    "                    < alpha\n",
    "                )\n",
    "                else HabitatSimActions.TURN_RIGHT\n",
    "            )\n",
    "            self._reset_agent_state(current_state)\n",
    "            return self._get_return_value(best_turn)\n",
    "\n",
    "    def _reset_agent_state(self, state: habitat_sim.AgentState) -> None:\n",
    "        self._sim.set_agent_state(\n",
    "            state.position, state.rotation, reset_sensors=False\n",
    "        )\n",
    "\n",
    "    def _geo_dist(self, goal_pos: np.array) -> float:\n",
    "        return self._sim.geodesic_distance(\n",
    "            self._sim.get_agent_state().position, [goal_pos]\n",
    "        )\n",
    "\n",
    "    def _est_max_grad_dir(self, goal_pos: np.array) -> np.array:\n",
    "\n",
    "        current_state = self._sim.get_agent_state()\n",
    "        current_pos = current_state.position\n",
    "\n",
    "        if self.mode == \"exact_gradient\":\n",
    "            points = self._sim.get_straight_shortest_path_points(\n",
    "                self._sim.get_agent_state().position, goal_pos\n",
    "            )\n",
    "#             print(\"Curr Pos: {}\".format(self._sim.get_agent_state().position))\n",
    "#             print(\"Goal Pos: {}\".format(goal_pos))\n",
    "#             print(\"Length of path: {}\".format(points))\n",
    "#             print(\"length: {}\".format(len(points)))\n",
    "            # Add a little offset as things get weird if\n",
    "            # points[1] - points[0] is anti-parallel with forward\n",
    "            if len(points) < 2:\n",
    "                return None\n",
    "            \n",
    "            max_grad_dir = quaternion_from_two_vectors(\n",
    "                self._sim.forward_vector,\n",
    "                points[1]\n",
    "                - points[0]\n",
    "                + EPSILON\n",
    "                * np.cross(self._sim.up_vector, self._sim.forward_vector),\n",
    "            )\n",
    "            max_grad_dir.x = 0\n",
    "            max_grad_dir = np.normalized(max_grad_dir)\n",
    "        else:\n",
    "            current_rotation = self._sim.get_agent_state().rotation\n",
    "            current_dist = self._geo_dist(goal_pos)\n",
    "\n",
    "            best_geodesic_delta = -2 * self._max_delta\n",
    "            best_rotation = current_rotation\n",
    "            for _ in range(0, 360, self._sim.config.TURN_ANGLE):\n",
    "                sim_action = HabitatSimActions.MOVE_FORWARD\n",
    "                self._sim.step(sim_action)\n",
    "                new_delta = current_dist - self._geo_dist(goal_pos)\n",
    "\n",
    "                if new_delta > best_geodesic_delta:\n",
    "                    best_rotation = self._sim.get_agent_state().rotation\n",
    "                    best_geodesic_delta = new_delta\n",
    "\n",
    "                # If the best delta is within (1 - cos(TURN_ANGLE))% of the\n",
    "                # best delta (the step size), then we almost certainly have\n",
    "                # found the max grad dir and should just exit\n",
    "                if np.isclose(\n",
    "                    best_geodesic_delta,\n",
    "                    self._max_delta,\n",
    "                    rtol=1 - np.cos(np.deg2rad(self._sim.config.TURN_ANGLE)),\n",
    "                ):\n",
    "                    break\n",
    "\n",
    "                self._sim.set_agent_state(\n",
    "                    current_pos,\n",
    "                    self._sim.get_agent_state().rotation,\n",
    "                    reset_sensors=False,\n",
    "                )\n",
    "\n",
    "                sim_action = HabitatSimActions.TURN_LEFT\n",
    "                self._sim.step(sim_action)\n",
    "\n",
    "            self._reset_agent_state(current_state)\n",
    "\n",
    "            max_grad_dir = best_rotation\n",
    "            \n",
    "        return max_grad_dir\n",
    "\n",
    "    @property\n",
    "    def mode(self):\n",
    "        return self._mode\n",
    "\n",
    "    @mode.setter\n",
    "    def mode(self, new_mode: str):\n",
    "        r\"\"\"Sets the mode for how the greedy follower determines the gradient of the geodesic_distance\n",
    "        function (it then follows the negative of this gradient)\n",
    "\n",
    "        :param new_mode: Must be one of 'exact_gradient' or 'approximate_gradient'.  If 'exact_gradient', then\n",
    "                         the simulator must provide a straightend shortest path.\n",
    "\n",
    "                         If 'approximate_gradient', the follower approximates the gradient by turning\n",
    "                         and then stepping forward\n",
    "        \"\"\"\n",
    "        assert new_mode in {\"exact_gradient\", \"approximate_gradient\"}\n",
    "        if new_mode == \"exact_gradient\":\n",
    "            assert (\n",
    "                getattr(self._sim, \"get_straight_shortest_path_points\", None)\n",
    "                is not None\n",
    "            )\n",
    "        self._mode = new_mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(obs):\n",
    "    color_obs = obs[\"rgb\"][:, :, :3]\n",
    "    color_obs[190:195, 125:130, :] = [0, 0, 255] \n",
    "    depth_obs = obs[\"depth\"]\n",
    "    depth_obs = ((depth_obs - np.min(depth_obs))/ np.max(depth_obs) * 255.0).astype(np.int)\n",
    "    depth_obs = np.stack([depth_obs]*3, axis=2)[:, :, :, 0]\n",
    "    plt.imshow(np.concatenate([color_obs, depth_obs], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_object(sim, object_id):\n",
    "    sim.set_object_motion_type(MotionType.DYNAMIC, object_id)\n",
    "    position = np.array(sim.get_translation(object_id))\n",
    "    trans = sim.get_transformation(object_id)\n",
    "    relative_trans = trans.transform_point(mn.Vector3([0, 1, 0]))\n",
    "    sim.set_translation(relative_trans, object_id)\n",
    "    sim.recompute_navmesh(sim.pathfinder, sim.navmesh_settings, True)\n",
    "    new_position = np.array(sim.get_translation(object_id))\n",
    "    \n",
    "    return position, new_position\n",
    "\n",
    "def reset_object(sim, object_id, position):\n",
    "    sim.set_translation(position, object_id)\n",
    "    sim.recompute_navmesh(sim.pathfinder, sim.navmesh_settings, True)\n",
    "    sim.set_object_motion_type(MotionType.STATIC, object_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_example(config, env, mode):\n",
    "    object_goal_radius = 1.0\n",
    "    object_follower = ShortestPathFollower(env.habitat_env.sim, object_goal_radius, False)\n",
    "    object_follower.mode = mode\n",
    "    habitat_sim_obj = env.habitat_env.sim\n",
    "    \n",
    "    goal_radius = 0.75\n",
    "    goal_follower = ShortestPathFollower(env.habitat_env.sim, goal_radius, False)\n",
    "    goal_follower.mode = mode\n",
    "\n",
    "    demonstrations = []\n",
    "    for episode in range(len(env.habitat_env.episodes)):\n",
    "        try:\n",
    "            observations = env.reset()\n",
    "            dem = {}\n",
    "            dem['episode_id'] = env.habitat_env.current_episode.episode_id\n",
    "            dem['actions'] = []\n",
    "            dem['is_grab_success'] = True\n",
    "            # print(dem['episode_id'])\n",
    "            images = []\n",
    "            \n",
    "            position, new_position = move_object(habitat_sim_obj._sim, 1)\n",
    "            \n",
    "            while not env.habitat_env.episode_over:\n",
    "                # position = np.array(env.habitat_env.sim._sim.get_translation(1))\n",
    "                # position = env.habitat_env.current_episode.goals[0].position\n",
    "                \n",
    "                best_action = object_follower.get_next_action(\n",
    "                    position\n",
    "                )\n",
    "                \n",
    "                if best_action is None:                \n",
    "                    break \n",
    "\n",
    "                dem['actions'].append(env.habitat_env.task.get_action_name(best_action))\n",
    "                observations, reward, done, infos = env.step(action={'action': best_action})\n",
    "                # print(observations)\n",
    "                im = observations[\"rgb\"]\n",
    "                im[190:195, 125:130, :] = [0, 0, 255] \n",
    "                images.append(im)\n",
    "            \n",
    "            reset_object(habitat_sim_obj._sim, 1, position)\n",
    "            observations = habitat_sim_obj.get_observations_at()\n",
    "            im = observations[\"rgb\"]\n",
    "            im[190:195, 125:130, :] = [0, 0, 255] \n",
    "            images.append(im)\n",
    "\n",
    "            # Call grab action\n",
    "            observations, reward, done, infos = env.step(action={'action':4})\n",
    "            im = observations[\"rgb\"]\n",
    "            im[190:195, 125:130, :] = [0, 0, 255] \n",
    "            images.append(im)\n",
    "\n",
    "            dem['actions'].append(env.habitat_env.task.get_action_name(4))\n",
    "            success = observations['gripped_object_id'] != -1\n",
    "\n",
    "            if not success:\n",
    "                dem['is_grab_success'] = False\n",
    "                # images_to_video(images, 'data/videos', 'trajectory' + str(episode))\n",
    "                # dem['videos'] = 'data/videos/trajectories' + str(episode)\n",
    "                dem['infos'] = infos\n",
    "                dem['reward'] = reward\n",
    "                dem['success'] = False\n",
    "                dem['done'] = done\n",
    "\n",
    "                yield False, observations, reward, done, infos, dem, images\n",
    "\n",
    "            while not env.habitat_env.episode_over:\n",
    "                position = np.array(env.habitat_env.sim._sim.get_translation(1))\n",
    "                best_action = goal_follower.get_next_action(\n",
    "                    env.habitat_env.current_episode.goals[0].position\n",
    "                )\n",
    "                if best_action is None:\n",
    "                    best_action = 4\n",
    "                    dem['actions'].append(env.habitat_env.task.get_action_name(best_action))\n",
    "                    observations, reward, done, infos = env.step(action={'action': best_action})\n",
    "                    im = observations[\"rgb\"]\n",
    "                    im[190:195, 125:130, :] = [0, 0, 255] \n",
    "                    images.append(im)\n",
    "                    break\n",
    "\n",
    "                dem['actions'].append(env.habitat_env.task.get_action_name(best_action))\n",
    "                observations, reward, done, infos = env.step(action={'action': best_action})\n",
    "                im = observations[\"rgb\"]\n",
    "                im[190:195, 125:130, :] = [0, 0, 255] \n",
    "                images.append(im)\n",
    "\n",
    "            # dem['videos'] = 'data/videos/trajectories' + str(episode)\n",
    "            dem['infos'] = infos\n",
    "            dem['reward'] = reward\n",
    "            dem['success'] = True\n",
    "            dem['done'] = done\n",
    "\n",
    "            # images_to_video(images, 'data/videos', 'trajectory' + str(episode))\n",
    "            yield True, observations, reward, done, infos, dem, images\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            yield False, observations, None, done, infos, dem, images\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1baceacf4cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-30 18:47:44,729 initializing sim SokobanSim-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-30 18:47:46,784 Initializing task Sokoban-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Agents\n",
      "Initializing Objects\n",
      "Possible Actions: OrderedDict([('STOP', <habitat.tasks.nav.nav.StopAction object at 0x7fedcbdf90f0>), ('MOVE_FORWARD', <habitat.tasks.nav.nav.MoveForwardAction object at 0x7fedcbdf94e0>), ('TURN_LEFT', <habitat.tasks.nav.nav.TurnLeftAction object at 0x7fedcbdf91d0>), ('TURN_RIGHT', <habitat.tasks.nav.nav.TurnRightAction object at 0x7fedcbdf9748>), ('GRAB_RELEASE', <habitat.tasks.sokoban.sokoban_task.GrabOrReleaseAction object at 0x7fedcbdf9400>)])\n"
     ]
    }
   ],
   "source": [
    "env = SokobanRLEnv(config=config, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----, id:1, success: True, count=0, done=True, dist_to_goal: {0: 0.4593968100827051}\n",
      "2\n",
      "----\n",
      "----, id:2, success: True, count=1, done=False, dist_to_goal: {0: 0.274644216669889, 1: 7.0357391772669535}\n",
      "3\n",
      "----\n",
      "----, id:3, success: True, count=2, done=False, dist_to_goal: {1: 2.179882833929407, 0: 2.1289546141440887}\n",
      "4\n",
      "----\n",
      "----, id:4, success: True, count=3, done=False, dist_to_goal: {1: 4.1468295693248995, 0: 2.820923131170276}\n",
      "5\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "demonstrations = []\n",
    "episode_images = []\n",
    "for i, (val, obs, reward, done, infos, dem, images) in enumerate(shortest_path_example(config.TASK_CONFIG, env, \"approximate_gradient\")):\n",
    "    if val == False or done==False:\n",
    "        count += 1\n",
    "        print('----')\n",
    "        print(dem['episode_id'])\n",
    "        print('----')\n",
    "        images_to_video(images, 'data/videos', 'trajectory'+str(dem['episode_id']))\n",
    "    # print(obs)\n",
    "    # print(obs['gripped_object_id'], infos, done, reward)\n",
    "    print(\"i: {}, id:{}, success: {}, count={}, done={}, dist_to_goal: {}\".format(\n",
    "        i, dem['episode_id'], val, count, done, infos['object_distance_to_goal']\n",
    "    ), end=\"\\r\")\n",
    "    episode_images.append(images)\n",
    "    demonstrations.append(dem)\n",
    "    \n",
    "    # plt.imshow(images[-1])\n",
    "    \n",
    "    # images_to_video(images, 'data/videos', 'trajectory'+str(dem['episode_id']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential issues: \n",
    "# will not handle geodesic distances really well. #7743\n",
    "# will not handle cases where the object is spawned too close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrations_dict = {}\n",
    "for demo in demonstrations:\n",
    "    if 'infos' in demo:\n",
    "        if 'object_goals' in demo['infos']:\n",
    "            demo['infos']['object_goals'][1] = np.array(demo['infos']['object_goals'][1]).tolist()\n",
    "        \n",
    "    demonstrations_dict[demo['episode_id']] = demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/datasets/sokoban/coda/v1/train/content/coda_hard.json.gz', \"rt\") as f:\n",
    "    episodes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'actions', 'is_grab_success', 'success', 'done'\n",
    "for episode_id,demo  in demonstrations_dict.items():\n",
    "    episode_id = demo['episode_id']\n",
    "    episodes['episodes'][int(episode_id)]['demonstrations'] = demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/datasets/sokoban/coda/v1/train/content/coda_hard_wdemo.json.gz', \"wt\") as f:\n",
    "    json.dump(episodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
